name: Raw Data Async Sync

on:
  workflow_run:
    workflows: ["Horizon Collector"]
    types: [completed]
  schedule:
    - cron: "45 */6 * * *"
  workflow_dispatch:
    inputs:
      max_items:
        description: "Optional cap for this run (0 = no cap)"
        required: false
        default: "100"

permissions:
  contents: read

concurrency:
  group: raw-data-async
  cancel-in-progress: false

env:
  METADATA_CSV: Data/raw/Metadata/shorts_metadata_horizon.csv
  RAW_PREFIX: clipfarm/raw
  STATE_PREFIX: clipfarm/state
  VIDEO_PREFIX: video
  AUDIO_PREFIX: audio
  TEXT_PREFIX: text

jobs:
  video_audio:
    if: >-
      github.event_name != 'workflow_run' ||
      (github.event.workflow_run.conclusion == 'success' && github.event.workflow_run.head_branch == 'main')
    runs-on: ubuntu-latest
    env:
      S3_BUCKET: ${{ secrets.S3_BUCKET }}
      MAX_ITEMS: ${{ github.event.inputs.max_items || '100' }}
      YTDLP_COOKIES_TXT: ${{ secrets.YTDLP_COOKIES_TXT }}
      YTDLP_COOKIES_B64: ${{ secrets.YTDLP_COOKIES_B64 }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # Use branch tip (not event SHA) so metadata CSV committed by Horizon Collector
          # is visible to this workflow run.
          ref: ${{ github.event.workflow_run.head_branch || github.ref_name }}
          fetch-depth: 0

      - name: Validate required secrets
        run: |
          if [ -z "${{ secrets.AWS_ACCESS_KEY_ID }}" ] || [ -z "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ] || [ -z "${{ secrets.AWS_REGION }}" ] || [ -z "$S3_BUCKET" ]; then
            echo "Missing one or more required AWS secrets: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, S3_BUCKET"
            exit 1
          fi

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install yt-dlp requests boto3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Restore video state
        run: |
          mkdir -p state
          aws s3 cp "s3://$S3_BUCKET/${STATE_PREFIX}/video_downloader.sqlite" state/video_downloader.sqlite || true

      - name: Prepare yt-dlp cookies file (optional)
        run: |
          if [ -n "${YTDLP_COOKIES_B64:-}" ]; then
            printf "%s" "$YTDLP_COOKIES_B64" | base64 -d > /tmp/youtube_cookies.txt
          elif [ -n "${YTDLP_COOKIES_TXT:-}" ]; then
            printf "%s\n" "$YTDLP_COOKIES_TXT" > /tmp/youtube_cookies.txt
          fi

          if [ -f /tmp/youtube_cookies.txt ]; then
            echo "cookies_file_present=yes"
            echo "cookies_line_count=$(wc -l < /tmp/youtube_cookies.txt)"
            echo "cookies_header=$(head -n 1 /tmp/youtube_cookies.txt)"
          else
            echo "cookies_file_present=no"
          fi

      - name: Run video+audio downloader
        run: |
          CLOUD_ROOT_URI="s3://$S3_BUCKET/${RAW_PREFIX}"

          args=(
            --metadata_csv "$METADATA_CSV"
            --state_db state/video_downloader.sqlite
            --cloud_root_uri "$CLOUD_ROOT_URI"
            --cloud_video_prefix "$VIDEO_PREFIX"
            --cloud_audio_prefix "$AUDIO_PREFIX"
            --cloud_delete_local_after_upload
            --retry 1
            --sleep_interval 1.5
            --max_sleep_interval 4.0
          )

          if [ "$MAX_ITEMS" != "0" ]; then
            args+=(--max_items "$MAX_ITEMS")
          fi

          if [ -f /tmp/youtube_cookies.txt ]; then
            args+=(--cookies_file /tmp/youtube_cookies.txt)
          fi

          python Data/raw/Video/download_video.py "${args[@]}"

      - name: Persist video state
        if: always()
        run: |
          if [ -f state/video_downloader.sqlite ]; then
            aws s3 cp state/video_downloader.sqlite "s3://$S3_BUCKET/${STATE_PREFIX}/video_downloader.sqlite"
          fi

  text:
    needs: [video_audio]
    if: >-
      needs.video_audio.result == 'success' &&
      (
        github.event_name != 'workflow_run' ||
        (github.event.workflow_run.conclusion == 'success' && github.event.workflow_run.head_branch == 'main')
      )
    runs-on: ubuntu-latest
    env:
      S3_BUCKET: ${{ secrets.S3_BUCKET }}
      MAX_ITEMS: ${{ github.event.inputs.max_items || '100' }}
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      YTDLP_COOKIES_TXT: ${{ secrets.YTDLP_COOKIES_TXT }}
      YTDLP_COOKIES_B64: ${{ secrets.YTDLP_COOKIES_B64 }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.workflow_run.head_branch || github.ref_name }}
          fetch-depth: 0

      - name: Validate required secrets
        run: |
          if [ -z "${{ secrets.AWS_ACCESS_KEY_ID }}" ] || [ -z "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ] || [ -z "${{ secrets.AWS_REGION }}" ] || [ -z "$S3_BUCKET" ]; then
            echo "Missing one or more required AWS secrets: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, S3_BUCKET"
            exit 1
          fi
          if [ -z "$OPENAI_API_KEY" ]; then
            echo "Missing OPENAI_API_KEY secret"
            exit 1
          fi

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install yt-dlp requests openai boto3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Restore text state
        run: |
          mkdir -p state
          aws s3 cp "s3://$S3_BUCKET/${STATE_PREFIX}/text_downloader.sqlite" state/text_downloader.sqlite || true

      - name: Prepare yt-dlp cookies file (optional)
        run: |
          if [ -n "${YTDLP_COOKIES_B64:-}" ]; then
            printf "%s" "$YTDLP_COOKIES_B64" | base64 -d > /tmp/youtube_cookies.txt
          elif [ -n "${YTDLP_COOKIES_TXT:-}" ]; then
            printf "%s\n" "$YTDLP_COOKIES_TXT" > /tmp/youtube_cookies.txt
          fi

          if [ -f /tmp/youtube_cookies.txt ]; then
            echo "cookies_file_present=yes"
            echo "cookies_line_count=$(wc -l < /tmp/youtube_cookies.txt)"
            echo "cookies_header=$(head -n 1 /tmp/youtube_cookies.txt)"
          else
            echo "cookies_file_present=no"
          fi

      - name: Run text collector
        run: |
          CLOUD_ROOT_URI="s3://$S3_BUCKET/${RAW_PREFIX}"

          args=(
            --metadata_csv "$METADATA_CSV"
            --state_db state/text_downloader.sqlite
            --cloud_root_uri "$CLOUD_ROOT_URI"
            --cloud_audio_prefix "$AUDIO_PREFIX"
            --cloud_text_prefix "$TEXT_PREFIX"
            --cloud_delete_local_after_upload
            --asr_backend openai_api
            --asr_model whisper-1
          )

          if [ "$MAX_ITEMS" != "0" ]; then
            args+=(--max_items "$MAX_ITEMS")
          fi

          if [ -f /tmp/youtube_cookies.txt ]; then
            args+=(--cookies_file /tmp/youtube_cookies.txt)
          fi

          python Data/raw/Text/text_collect.py "${args[@]}"

      - name: Persist text state
        if: always()
        run: |
          if [ -f state/text_downloader.sqlite ]; then
            aws s3 cp state/text_downloader.sqlite "s3://$S3_BUCKET/${STATE_PREFIX}/text_downloader.sqlite"
          fi
