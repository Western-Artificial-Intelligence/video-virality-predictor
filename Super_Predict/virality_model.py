
"""
virality_model.py

PURPOSE (Dev 3 — Virality Prediction / Supervised)
==================================================

This module is the "first usable" supervised-learning layer on top of your project’s
multimodal embedding pipeline.

By the time Dev 3 starts, you already have:
  1) A list of YouTube Shorts (video IDs + metadata somewhere)
  2) Separate embeddings for Video / Audio / Metadata / Text
  3) A fused embedding per video (the file `fused.pt`)

What this module adds:
  - A supervised model that predicts a chosen performance metric (your "virality" proxy)
  - A consistent training pipeline (preprocessing + model + evaluation)
  - A simple inference wrapper: `predict_score(video_id, extra_features)`

WHY this step is necessary
--------------------------
Unsupervised steps (dimensionality reduction + clustering) help you *understand* the space
and discover "styles". But they do not directly answer:
    "Given a new Short, how well will it perform?"

Virality prediction is supervised learning: we need *labels* (targets) such as:
  - views at day 7
  - log(views_7d + 1)
  - engagement rate
  - watch time
  - or some internal KPI

This file makes it easy to:
  - start with just (embedding + target) as soon as you have any labels,
  - then later incorporate extra signals (cluster IDs, metadata, early engagement signals)
    without having to rewrite the model code.

WHAT it does (high level)
-------------------------
1) Loads fused embeddings from `fused.pt`.
2) Converts embeddings into a tabular format (DataFrame columns emb_0..emb_(d-1)).
3) Merges these rows with your labels/features table using `video_id`.
4) Automatically detects numeric vs categorical columns.
5) Builds an sklearn Pipeline:
     - imputes missing values
     - scales numeric features
     - one-hot encodes categorical features
     - trains a regressor
6) Evaluates the model on a held-out split.
7) Computes permutation feature importance to help explain what matters most.
8) Provides a `ViralityPredictor` wrapper for per-video inference.

WHEN to use this module
-----------------------
Use this file when you have *any* ground-truth target per video_id, even if minimal:
  - a CSV export from analytics
  - a dataset table in your warehouse
  - a joined dataframe from your ingestion pipeline

It is designed to be used:
  - early, for a baseline model and quick feedback loops
  - later, as a stable interface while you swap models (GBDT, Ridge, XGBoost, etc.)

ABOUT variables & where they come from
--------------------------------------
- `fused_pt_path`:
    Path to the `fused.pt` generated by your embedding fusion step (Dev pipeline step 3).
    In this repo, the user uploaded it to: `/mnt/data/fused.pt`.

- `features_df` (labels/features table):
    This is NOT produced by this file. It comes from your data pipeline / analytics export.
    It must contain:
      - `video_id` (matching the ids in fused.pt)
      - `target_col` (chosen performance metric, e.g. log_views_7d)

    Optional columns in `features_df` can include:
      - metadata: duration_s, posted_hour, language, etc.
      - early signals: views_1h, likes_1h, retention_30s, etc.
      - cluster IDs: cluster_id from Dev2 output
    This module will automatically include them if present.

- `target_col`:
    The name of your target column in `features_df`. Set in `ViralityConfig`.

- model hyperparameters:
    The default model is a decent baseline for tabular + many features:
      HistGradientBoostingRegressor
    (A fast gradient-boosted tree implementation in scikit-learn.)
    You can swap this out in `ViralityConfig.model`.

NOTE about "virality"
---------------------
This file does not decide what "virality" means. You do.
You choose:
  - the target metric
  - the prediction horizon
  - the allowed features (to avoid leakage)

For example:
  - If target is views_7d, you may allow features like views_1h (early prediction),
    but you must not allow views_7d itself or features computed after day 7.
  - For a proper evaluation, you may want a time-based split later (train earlier uploads, test later).

This module starts with a random split by default for simplicity.

"""

from __future__ import annotations

# Standard library imports
from dataclasses import dataclass
from typing import Optional, Sequence, Tuple, Dict, Any, List

# Third-party imports
import numpy as np
import pandas as pd
import torch

# scikit-learn utilities for building a clean, production-style training pipeline.
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.inspection import permutation_importance
from sklearn.ensemble import HistGradientBoostingRegressor


# =============================================================================
# 1) Loading utilities
# =============================================================================

def load_fused_pt(pt_path: str) -> Tuple[List[str], np.ndarray]:
    """
    Load fused embeddings from the project's `fused.pt`.

    WHY
    ---------------
    The fusion step (your step 3) typically writes embeddings in a PyTorch-friendly format
    for speed and storage efficiency.

    WHAT we expect `fused.pt` to contain
    -----------------------------------
    In your current pipeline, `fused.pt` is produced by your fusion code and contains:
        torch.load(pt_path) -> {"ids": [..], "fused": Tensor[n, d]}

    - "ids": list of `video_id` (strings) in the same order as embeddings
    - "fused": a tensor of shape [n_videos, embedding_dim]

    WHERE these keys come from
    --------------------------
    They come from your fusion job / script. If the fusion code changes, this loader
    is the single place you update.

    RETURNS
    -------
    ids : List[str]
        Video identifiers. These are the join-key into your label/feature table.
    fused : np.ndarray (float32)
        Embedding matrix of shape (n, d).
    """
    obj = torch.load(pt_path, map_location="cpu")

    # The fusion job is expected to store embeddings under the key "fused"
    # and their corresponding IDs under "ids".
    ids = obj["ids"]
    fused = obj["fused"]

    # Convert PyTorch tensor -> numpy (standard format for sklearn)
    if isinstance(fused, torch.Tensor):
        fused = fused.detach().cpu().numpy()

    # Enforce float32 for consistent memory usage
    fused = np.asarray(fused, dtype=np.float32)
    return ids, fused


def embeddings_to_df(ids: Sequence[str], fused: np.ndarray, prefix: str = "emb_") -> pd.DataFrame:
    """
    Convert an embedding matrix into a "wide" pandas DataFrame.

    WHY
    ------------------
    scikit-learn models/pipelines usually train on tabular matrices.
    Converting to a DataFrame makes it very easy to:
      - merge embeddings with metadata/signals (via `video_id`)
      - track feature names for debugging and feature importance

    WHAT
    ----------------
    A DataFrame with columns:
        video_id, emb_0, emb_1, ... emb_{d-1}

    - `video_id` comes from the `ids` list (fusion output).
    - `emb_i` comes from the i-th dimension of the fused embedding.

    PARAMS
    ------
    ids : sequence of str
        Video IDs from `fused.pt`.
    fused : np.ndarray
        Shape (n_videos, d_embedding).
    prefix : str
        Prefix used to name embedding columns. Default "emb_".

    RETURNS
    -------
    pd.DataFrame
        Shape (n_videos, 1 + d_embedding).
    """
    d = fused.shape[1]
    cols = [f"{prefix}{i}" for i in range(d)]
    df = pd.DataFrame(fused, columns=cols)

    # Insert the ID column at position 0 so it’s obvious and easy to locate.
    df.insert(0, "video_id", list(ids))
    return df


def load_features_csv(csv_path: str) -> pd.DataFrame:
    """
    Load a CSV that contains labels (targets) and optionally extra features.

    WHY this is separate
    --------------------
    Labels/targets almost always come from a different pipeline than embeddings:
      - YouTube Analytics export
      - internal ETL
      - warehouse query
      - manual annotation

    The only required column is:
      - video_id

    You also need a target column (configured later via `ViralityConfig.target_col`),
    but we don't force a specific name here to keep things flexible.

    REQUIRED COLUMNS
    ----------------
    video_id : str
        Must match the IDs found in fused.pt.

    RETURNS
    -------
    pd.DataFrame
    """
    df = pd.read_csv(csv_path)
    if "video_id" not in df.columns:
        raise ValueError("features CSV must include a 'video_id' column.")
    return df


# =============================================================================
# 2) Model configuration
# =============================================================================

@dataclass
class ViralityConfig:
    """
    Configuration for training a virality model.

    target_col
    ----------
    Name of your target column in the labels/features table.
    Example choices:
      - "log_views_7d"
      - "views_7d"
      - "engagement_rate_7d"
      - "watch_time_7d"

    test_size / random_state
    ------------------------
    Default uses a random train/test split.
    For a more realistic evaluation, replace with time-based split later.

    model
    -----
    Default is HistGradientBoostingRegressor:
      - handles non-linear patterns
      - works well on tabular mixes (embeddings + metadata + signals)
      - fast enough for iteration
    You can swap in Ridge, RandomForest, XGBoost, etc. as you mature the system.

    categorical_cols
    ----------------
    Optional list of columns you want to force as categorical.
    This is especially useful when:
      - cluster_id is numeric but semantically categorical
      - channel_id is numeric but should be treated as category
    """
    target_col: str = "target"
    test_size: float = 0.2
    random_state: int = 42

    # A strong "baseline" for mixed tabular features.
    # - learning_rate/max_depth/max_iter control model capacity
    # - l2_regularization helps prevent overfitting
    model: Any = HistGradientBoostingRegressor(
        loss="squared_error",
        learning_rate=0.05,
        max_depth=6,
        max_iter=400,
        l2_regularization=1e-4,
        random_state=42,
    )

    categorical_cols: Optional[List[str]] = None


# =============================================================================
# 3) Preprocessing helpers
# =============================================================================

def _infer_feature_columns(df: pd.DataFrame, target_col: str) -> Tuple[List[str], List[str]]:
    """
    Infer which columns are numeric vs categorical.

    WHY we infer this
    -----------------
    Your `features_df` can evolve over time:
      - Dev2 adds `cluster_id`
      - someone adds duration, language, post time
      - someone adds early engagement signals
    If we hard-code column lists, the code becomes brittle.

    HOW we infer
    ------------
    1) Drop identifier (`video_id`) and the target column.
    2) Mark numeric types (int/float/bool) as numeric.
    3) Everything else becomes categorical (object, string, category).

    IMPORTANT: categorical forcing
    ------------------------------
    Some numeric columns should be treated as categorical (e.g., cluster_id).
    We handle that by casting those columns to category in `train_virality_model`
    (using config.categorical_cols).

    RETURNS
    -------
    num_cols : List[str]
        Columns treated as numeric.
    cat_cols : List[str]
        Columns treated as categorical (one-hot encoded).
    """
    feature_df = df.drop(columns=[c for c in ["video_id", target_col] if c in df.columns])

    # Numeric features: ints/floats/bools
    num_cols = [c for c in feature_df.columns if pd.api.types.is_numeric_dtype(feature_df[c])]

    # Categorical features: everything else
    cat_cols = [c for c in feature_df.columns if c not in num_cols]

    return num_cols, cat_cols


def build_pipeline(num_cols: List[str], cat_cols: List[str], model: Any) -> Pipeline:
    """
    Build an sklearn Pipeline: preprocessing + model.

    WHY pipelines matter
    --------------------
    - Ensures train-time and inference-time preprocessing are identical.
    - Makes it easy to save/load the entire process.
    - Prevents data leakage from preprocessing done "outside" training.

    PREPROCESSING STEPS
    -------------------
    Numeric columns:
      1) Median imputation: fills missing numeric values robustly.
      2) StandardScaler: scales features.
         - Not strictly required for tree models, but useful if you later switch
           to linear models.
         - with_mean=False keeps sparse-matrix compatibility, because one-hot
           encoded categorical features create sparse matrices.

    Categorical columns:
      1) Most-frequent imputation: fill missing categories.
      2) OneHotEncoder: turns categories into one-hot vectors.

    RETURNS
    -------
    sklearn.pipeline.Pipeline
        Ready to `.fit(X_train, y_train)` and `.predict(X_test)`.
    """
    # Numeric pipeline
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False)),
    ])

    # Categorical pipeline
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore")),
    ])

    # ColumnTransformer applies the right transformer to each column subset
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_cols),
            ("cat", categorical_transformer, cat_cols),
        ],
        remainder="drop",        # drop any columns we didn't declare
        sparse_threshold=0.3,    # encourage sparse representation when useful
    )

    pipe = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", model),
    ])
    return pipe


# =============================================================================
# 4) Training / evaluation
# =============================================================================

def train_virality_model(
    fused_pt_path: str,
    features_df: pd.DataFrame,
    config: ViralityConfig,
) -> Dict[str, Any]:
    """
    Train a baseline supervised virality model.

    WHAT you provide
    ----------------
    fused_pt_path:
      Path to `fused.pt` created by your embedding fusion step (step 3 in your pipeline).

    features_df:
      A DataFrame with at least:
        - video_id: join key
        - config.target_col: the target value you want to predict

      It can ALSO contain any other features you want the model to use:
        - cluster_id (from Dev2)
        - metadata (duration, post hour, language, etc.)
        - early engagement signals (views_1h, likes_1h, etc.)

      This makes Dev3 work "not blocked" by other devs:
        - start with video_id + target
        - later add more columns and re-train

    config:
      ViralityConfig with:
        - target_col name
        - model choice
        - splitting parameters
        - (optional) categorical columns to force

    STEPS performed
    ---------------
    1) Load embeddings from fused.pt
    2) Convert embeddings to a DataFrame
    3) Merge with features_df on video_id
         - This automatically drops videos that have embeddings but no labels,
           and labels that don't have embeddings.
    4) Optionally cast some columns to categorical (config.categorical_cols).
    5) Split into train/test sets.
    6) Infer numeric/categorical columns.
    7) Build preprocessing + model pipeline and train it.
    8) Evaluate MAE and R² on held-out set.
    9) Run permutation feature importance for interpretability.

    OUTPUT
    ------
    Returns a dict with:
      - "pipeline": trained sklearn Pipeline
      - "metrics": {mae, r2, n_train, n_test, n_total}
      - "feature_importance": DataFrame with importance_mean/std (permutation importance)
      - "columns": inferred num/cat columns
      - "train_ids" / "test_ids": lists of video_ids in each split

    IMPORTANT NOTES (team-wide)
    ---------------------------
    - Default split is random. For production, use a time split once you have upload_time.
    - If your target distribution is heavy-tailed (views), consider training on log(target+1).
    - Be careful about leakage: if target is views_7d, do not include signals measured after day 7.
    """
    # ---- Step 1: load embeddings
    ids, fused = load_fused_pt(fused_pt_path)

    # ---- Step 2: make embeddings tabular (video_id + emb_*)
    emb_df = embeddings_to_df(ids, fused)

    # ---- Step 3: merge embeddings with labels/features
    # 'inner' join ensures we only train on videos that have both embeddings AND labels.
    df = emb_df.merge(features_df, on="video_id", how="inner")

    if config.target_col not in df.columns:
        raise ValueError(f"features_df must include target column '{config.target_col}'")

    # ---- Step 4: optionally force some columns to categorical
    # Example: cluster_id is numeric but should be treated as category.
    if config.categorical_cols:
        for c in config.categorical_cols:
            if c in df.columns:
                df[c] = df[c].astype("category")

    # ---- Step 5: train/test split
    # X contains all features INCLUDING video_id (we keep it around for traceability).
    # The pipeline will drop/ignore it because it's not numeric/categorical feature,
    # but we also handle it explicitly in inference.
    X = df.drop(columns=[config.target_col])
    y = df[config.target_col].astype(float)

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=config.test_size,
        random_state=config.random_state,
    )

    # ---- Step 6: detect numeric vs categorical columns
    # We infer based on dtypes, which automatically adapts when new feature columns are added.
    # We pass a combined DataFrame purely to reuse the helper logic cleanly.
    num_cols, cat_cols = _infer_feature_columns(
        pd.concat([X, y.rename(config.target_col)], axis=1),
        config.target_col
    )

    # ---- Step 7: build pipeline and fit
    pipe = build_pipeline(num_cols=num_cols, cat_cols=cat_cols, model=config.model)
    pipe.fit(X_train, y_train)

    # ---- Step 8: evaluate
    preds = pipe.predict(X_test)
    metrics = {
        "mae": float(mean_absolute_error(y_test, preds)),
        "r2": float(r2_score(y_test, preds)),
        "n_train": int(len(X_train)),
        "n_test": int(len(X_test)),
        "n_total": int(len(df)),
    }

    # ---- Step 9: permutation feature importance (interpretability)
    # Permutation importance measures how much the metric degrades when a feature is shuffled.
    # It's model-agnostic and gives a first look at what matters.
    #
    # Caveat:
    # - Importance is computed on the processed feature space.
    # - For categorical features, one-hot encoding expands them into many columns.
    # - We expose those expanded names so you can still interpret them.
    try:
        r = permutation_importance(
            pipe, X_test, y_test,
            n_repeats=10,
            random_state=config.random_state,
            n_jobs=1
        )

        # Build readable feature names for the transformed space
        pre = pipe.named_steps["preprocess"]
        feature_names: List[str] = []

        # Numeric names remain the same
        if num_cols:
            feature_names.extend(num_cols)

        # Categorical names expand via OneHotEncoder
        if cat_cols:
            ohe = pre.named_transformers_["cat"].named_steps["onehot"]
            ohe_names = list(ohe.get_feature_names_out(cat_cols))
            feature_names.extend(ohe_names)

        # Create a sorted DataFrame
        fi = pd.DataFrame({
            "feature": feature_names[: len(r.importances_mean)],
            "importance_mean": r.importances_mean,
            "importance_std": r.importances_std,
        }).sort_values("importance_mean", ascending=False)

    except Exception:
        # If anything goes wrong (rare, but can happen if feature space is huge),
        # return an empty table rather than failing training.
        fi = pd.DataFrame(columns=["feature", "importance_mean", "importance_std"])

    return {
        "pipeline": pipe,
        "metrics": metrics,
        "feature_importance": fi,
        "columns": {"num_cols": num_cols, "cat_cols": cat_cols},

        # Keep IDs so you can inspect errors and pull examples.
        "train_ids": X_train["video_id"].tolist() if "video_id" in X_train.columns else [],
        "test_ids": X_test["video_id"].tolist() if "video_id" in X_test.columns else [],
    }


# =============================================================================
# 5) Inference wrapper (the "predict_score(video)" API)
# =============================================================================

class ViralityPredictor:
    """
    ViralityPredictor
    -----------------
    Small wrapper class that turns the trained pipeline into a simple API:

        predictor.predict_score(video_id, extra_features) -> float

    WHY this wrapper exists
    -----------------------
    In real systems, training code and inference code often diverge.
    The wrapper reduces that risk by:
      - reusing the same preprocessing+model pipeline (sklearn Pipeline)
      - ensuring embeddings are constructed identically every time

    It also standardizes the interface Dev3 promised the team:
      - "Wrap it into a simple predict_score(video) function"

    HOW it works
    ------------
    - At initialization we store:
        - the trained pipeline
        - a mapping from video_id -> embedding vector
    - For inference:
        - fetch the embedding for that video_id
        - create a single-row DataFrame with emb_0..emb_(d-1)
        - attach any extra features provided (cluster_id, early signals, etc.)
        - call pipeline.predict on that one row

    IMPORTANT:
    ----------
    If you pass extra_features at inference time, they must have the same column names
    that were available during training (or the pipeline will ignore them / error depending on transformer).
    """

    def __init__(self, pipeline: Pipeline, embedding_map: Dict[str, np.ndarray], emb_prefix: str = "emb_"):
        """
        Parameters
        ----------
        pipeline:
            A trained sklearn Pipeline returned by train_virality_model(...).
            It includes preprocessing + model.

        embedding_map:
            Dict mapping video_id -> fused embedding vector (numpy array).
            This is derived from fused.pt.

        emb_prefix:
            Column prefix used to name embedding features. Must match training.
        """
        self.pipeline = pipeline
        self.embedding_map = embedding_map
        self.emb_prefix = emb_prefix

        # Placeholder if you later want to enforce strict input schema
        self.expected_columns = None

    @staticmethod
    def from_fused_pt(pipeline: Pipeline, fused_pt_path: str) -> "ViralityPredictor":
        """
        Convenience constructor: build embedding_map from fused.pt.

        This is the typical way the team will initialize the predictor after training.

        Example
        -------
            out = train_virality_model(...)
            predictor = ViralityPredictor.from_fused_pt(out["pipeline"], "/path/to/fused.pt")
        """
        ids, fused = load_fused_pt(fused_pt_path)
        emb_map = {vid: fused[i] for i, vid in enumerate(ids)}
        return ViralityPredictor(pipeline=pipeline, embedding_map=emb_map)

    def _row_from_inputs(self, video_id: str, extra_features: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
        """
        Internal helper that creates a single-row DataFrame for inference.

        WHY we must build a row like this
        --------------------------------
        The sklearn pipeline expects the same "column layout" used at training.
        Training used a DataFrame with:
            video_id, emb_0..emb_(d-1), ...other feature columns...

        So we recreate that same structure here.
        """
        if video_id not in self.embedding_map:
            raise KeyError(f"Unknown video_id: {video_id}")

        emb = self.embedding_map[video_id]

        # Start with the identifier (kept for traceability).
        row: Dict[str, Any] = {"video_id": video_id}

        # Add each embedding dimension as its own feature column.
        for i, v in enumerate(emb):
            row[f"{self.emb_prefix}{i}"] = float(v)

        # Attach any optional extra features (cluster_id, early signals, metadata, etc.)
        if extra_features:
            row.update(extra_features)

        return pd.DataFrame([row])

    def predict_score(self, video_id: str, extra_features: Optional[Dict[str, Any]] = None) -> float:
        """
        Public inference API: predict a single score for a video.

        Parameters
        ----------
        video_id:
            Must exist in the embeddings file used to build embedding_map (fused.pt).

        extra_features:
            Optional dict of additional features.
            Use this to add:
              - early engagement signals (views_1h, likes_1h, etc.)
              - metadata (duration_s, posted_hour, language, etc.)
              - cluster_id (from Dev2)

            IMPORTANT:
              - These keys must match column names seen in training.
              - If a key is unseen and your pipeline drops unknown columns, it may be ignored.
                If it errors, you need to re-train with that feature included.

        Returns
        -------
        float
            The model's predicted value in the same units as the target.
            Example: if target is log(views_7d+1), prediction is also log-scale.
        """
        X = self._row_from_inputs(video_id, extra_features=extra_features)
        yhat = float(self.pipeline.predict(X)[0])
        return yhat
